from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
import os

# Initialize the FastAPI app
app = FastAPI()

# --- AI Model Loading ---
# This section loads the model files that will be deployed alongside the code.
# We will create these placeholder files for now.
model = None
vectorizer = None

@app.on_event("startup")
def load_model():
    global model, vectorizer
    # In a real scenario, these files would be generated by our ML pipeline.
    # For now, we create dummy placeholders so the app can start.
    if not os.path.exists("verifai_model.joblib"):
        # Create a dummy model and vectorizer
        dummy_vectorizer = TfidfVectorizer()
        dummy_vectorizer.fit_transform(["authentic", "counterfeit"])
        joblib.dump(dummy_vectorizer, "verifai_vectorizer.joblib")

        dummy_model = lambda: None
        setattr(dummy_model, 'predict', lambda x: ["authentic"])
        joblib.dump(dummy_model, "verifai_model.joblib")

    print("Loading model and vectorizer...")
    model = joblib.load("verifai_model.joblib")
    vectorizer = joblib.load("verifai_vectorizer.joblib")
    print("Model and vectorizer loaded successfully.")


# --- API Data Models ---
class VerificationRequest(BaseModel):
    image: str  # We won't use the image data yet, but it's in the contract
    object_class: str

class VerificationResponse(BaseModel):
    status: str
    title: str
    confidence: float
    summary: str
    details: list

# --- API Endpoint ---
@app.post("/verify", response_model=VerificationResponse)
def verify_item(request: VerificationRequest):
    try:
        # Simulate feature extraction from the object class
        # A real model would use features from the image itself.
        features = vectorizer.transform([request.object_class])

        # Get a prediction from our loaded model
        prediction = model.predict(features)
        predicted_label = prediction[0]

        print(f"Received request for '{request.object_class}'. Prediction: '{predicted_label}'")

        # Create a response based on the prediction
        return {
            "status": "verified" if predicted_label == "authentic" else "warning",
            "title": f"Live Verification for {request.object_class.title()}",
            "confidence": 0.93, # Using a static confidence for this example
            "summary": f"Self-hosted AI model predicted this item is {predicted_label}.",
            "details": [
                {"agent": "Render-Hosted Model", "finding": f"Prediction output: {predicted_label}", "status": "success"}
            ]
        }
    except Exception as e:
        print(f"Error during prediction: {e}")
        raise HTTPException(status_code=500, detail="An internal error occurred during AI analysis.")

@app.get("/")
def read_root():
    return {"message": "VerifAi Backend is running."}