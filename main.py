from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
import os
# --- NEW: Import CORS middleware ---
from fastapi.middleware.cors import CORSMiddleware

# Initialize the FastAPI app
app = FastAPI()

# --- NEW: Add CORS Middleware ---
# This allows your frontend (running on a different domain) to communicate
# with this backend. This is a critical fix for web applications.
origins = [
    "*"  # Allows all origins for simplicity. For production, you'd list specific domains.
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- FIX: Define a simple class for our dummy model instead of a lambda ---
class DummyModel:
    def predict(self, features):
        # This mock model will always predict "authentic"
        return ["authentic"]

# --- AI Model Loading ---
# This section loads the model files that will be deployed alongside the code.
model = None
vectorizer = None

@app.on_event("startup")
def load_model():
    global model, vectorizer
    # In a real scenario, these files would be generated by our ML pipeline.
    # For now, we create dummy placeholders so the app can start.
    if not os.path.exists("verifai_model.joblib"):
        print("Creating dummy model files...")
        # Create a dummy vectorizer
        dummy_vectorizer = TfidfVectorizer()
        dummy_vectorizer.fit_transform(["authentic", "counterfeit"])
        joblib.dump(dummy_vectorizer, "verifai_vectorizer.joblib")

        # Create and save an instance of our dummy model class
        dummy_model_instance = DummyModel()
        joblib.dump(dummy_model_instance, "verifai_model.joblib")

    print("Loading model and vectorizer...")
    model = joblib.load("verifai_model.joblib")
    vectorizer = joblib.load("verifai_vectorizer.joblib")
    print("Model and vectorizer loaded successfully.")


# --- API Data Models ---
class VerificationRequest(BaseModel):
    image: str
    object_class: str

class VerificationResponse(BaseModel):
    status: str
    title: str
    confidence: float
    summary: str
    details: list

# --- API Endpoint ---
@app.post("/verify", response_model=VerificationResponse)
def verify_item(request: VerificationRequest):
    try:
        # Simulate feature extraction from the object class
        features = vectorizer.transform([request.object_class])
        
        # Get a prediction from our loaded model
        prediction = model.predict(features)
        predicted_label = prediction[0]
        
        print(f"Received request for '{request.object_class}'. Prediction: '{predicted_label}'")

        # Create a response based on the prediction
        return {
            "status": "verified" if predicted_label == "authentic" else "warning",
            "title": f"Live Verification for {request.object_class.title()}",
            "confidence": 0.93,
            "summary": f"Self-hosted AI model predicted this item is {predicted_label}.",
            "details": [
                {"agent": "Render-Hosted Model", "finding": f"Prediction output: {predicted_label}", "status": "success"}
            ]
        }
    except Exception as e:
        print(f"Error during prediction: {e}")
        raise HTTPException(status_code=500, detail="An internal error occurred during AI analysis.")

@app.get("/")
def read_root():
    return {"message": "VerifAi Backend is running."}
